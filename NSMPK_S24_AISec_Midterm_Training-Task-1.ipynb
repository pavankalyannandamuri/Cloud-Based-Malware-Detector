{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esNWMzKrVuWc"
      },
      "source": [
        "**Revised on 3/5/2024: Changed source files**\n",
        "\n",
        "This is the skeleton code for Task 1 of the midterm project. The files that are downloaded in step 4 are based on the [Ember 2018 dataset](https://arxiv.org/abs/1804.04637), and contain the features (and corresponding labels) extracted from 1 million PE files, split into 80\\% training and 20\\% test datasets. The code used for for feature extraction is available [here](https://colab.research.google.com/drive/16q9bOlCmnTquPtVXVzxUj4ZY1ORp10R2?usp=sharing). However, the preprocessing and featurization process may take up to 3 hours on Google Colab. Hence, I recommend using the processed datasets (Step 4) to speed up your development.\n",
        "\n",
        "Also, note that there is a new optional step 8.5 - To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets.\n",
        "\n",
        "**Step 1:** Mount your Google Drive by clicking on \"Mount Drive\" in the Files section (panel to the left of this text.)\n",
        "\n",
        "**Step 2:** Go to Runtime -> Change runtime type and select T4 GPU.\n",
        "\n",
        "**Step 3:** Create a folder in your Google Drive, and rename it to \"vMalConv\"\n",
        "\n",
        "**Step 4:** Download the pre-processed training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "eUq_FZwmZegw",
        "outputId": "0d3f5350-d193-4b70-b708-d73891aa3b2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# ~8GB\\n!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\\n!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\\n!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat\\n!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat\\n!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"# ~8GB\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ_JdZKfG7Q-",
        "outputId": "bc5d9705-12a0-4ed5-8bdb-9e79199d3188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V958PbDW3H0"
      },
      "source": [
        "**Step 5:** Copy the downloaded files to vMalConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "llip77F3amma",
        "outputId": "10f573a7-736b-41cc-adba-bdfd4da869b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!cp /content/X_train.dat /content/drive/MyDrive/vMalConv/X_train.dat\\n!cp /content/X_test.dat /content/drive/MyDrive/vMalConv/X_test.dat\\n!cp /content/y_train.dat /content/drive/MyDrive/vMalConv/y_train.dat\\n!cp /content/y_test.dat /content/drive/MyDrive/vMalConv/y_test.dat\\n!cp /content/metadata.csv /content/drive/MyDrive/vMalConv/metadata.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\"\"\"!cp /content/X_train.dat /content/drive/MyDrive/vMalConv/X_train.dat\n",
        "!cp /content/X_test.dat /content/drive/MyDrive/vMalConv/X_test.dat\n",
        "!cp /content/y_train.dat /content/drive/MyDrive/vMalConv/y_train.dat\n",
        "!cp /content/y_test.dat /content/drive/MyDrive/vMalConv/y_test.dat\n",
        "!cp /content/metadata.csv /content/drive/MyDrive/vMalConv/metadata.csv\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbRilyqTXnrE"
      },
      "source": [
        "**Step 6:** Download and install Ember:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76bc7PEmlwKB",
        "outputId": "867c63bc-a80c-4136-f3a9-d4ffdec42954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PFGimenez/ember.git\n",
            "  Cloning https://github.com/PFGimenez/ember.git to /tmp/pip-req-build-z09tznsq\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/PFGimenez/ember.git /tmp/pip-req-build-z09tznsq\n",
            "  Resolved https://github.com/PFGimenez/ember.git to commit 3b82fe63069884882e743af725d29cc2a67859f1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ember\n",
            "  Building wheel for ember (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ember: filename=ember-0.1.0-py3-none-any.whl size=13050 sha256=bb0187cd10cc4e927b711bd57d03a5e9442b33578344ed71a753f6dd208827bc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vxqeatua/wheels/8f/69/f9/1917c8df03b25fe53e8e2f6cb2c9f61a43dec179b19b10ab9f\n",
            "Successfully built ember\n",
            "Installing collected packages: ember\n",
            "Successfully installed ember-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/PFGimenez/ember.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aRVMSwCQT7D",
        "outputId": "70b8fa54-fffe-410d-8031-0471bb2c209c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lief\n",
            "  Downloading lief-0.14.1-cp310-cp310-manylinux_2_28_x86_64.manylinux_2_27_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lief\n",
            "Successfully installed lief-0.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install lief"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXym5qd8Yv8f"
      },
      "source": [
        "**Step 7:** Read vectorized features from the data files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfcHyoTsmCFH",
        "outputId": "f8cc5782-41d9-4ca1-bc7f-f7bdca364ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
            "WARNING:   lief version 0.14.1-bae887e0 found instead. There may be slight inconsistencies\n",
            "WARNING:   in the feature calculations.\n"
          ]
        }
      ],
      "source": [
        "import ember\n",
        "X_train, y_train, X_test, y_test = ember.read_vectorized_features(\"drive/MyDrive/vMalConv/\")\n",
        "metadata_dataframe = ember.read_metadata(\"drive/MyDrive/vMalConv/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTRCz7m7Z7EH"
      },
      "source": [
        "**Step 8:** Get rid of rows with no labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zj63lcvin44q"
      },
      "outputs": [],
      "source": [
        "labelrows = (y_train != -1)\n",
        "X_train = X_train[labelrows]\n",
        "y_train = y_train[labelrows]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sJeajcNh8DJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e357724-0471-4e88-c932-2fcb6107c040"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 2381)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "X_train = X_train[:60000]\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "i2bH-rnz8JWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84732734-2230-47ea-bb98-ff4c239b0db0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "y_train = y_train[:60000]\n",
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mVG59AGooyC5"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "h5f = h5py.File('X_train.h5', 'w')\n",
        "h5f.create_dataset('X_train', data=X_train)\n",
        "h5f.close()\n",
        "h5f = h5py.File('y_train.h5', 'w')\n",
        "h5f.create_dataset('y_train', data=y_train)\n",
        "h5f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5tmUIJNvpZch"
      },
      "outputs": [],
      "source": [
        "!cp /content/X_train.h5 /content/drive/MyDrive/vMalConv/X_train.h5\n",
        "!cp /content/y_train.h5 /content/drive/MyDrive/vMalConv/y_train.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKoXSzp59RN-"
      },
      "source": [
        "**Optional Step 8.5:** To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets. You can use the [Pandas Dataframe sample() method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html), or come up with your own sampling methodology. Be mindful of the fact that the database is heavily imbalanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-X9wwv_n9QkY"
      },
      "outputs": [],
      "source": [
        "### Your code (optional) for sampling the original dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1bRlBWlaQdd"
      },
      "source": [
        "> **Task 1:** Complete the following code to build the architecture of MalConv in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TvR5tuT-IiY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e6e4fb3-5818-4b55-b396-6b770c956173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MalConv(\n",
            "  (embed): Embedding(3000000, 8)\n",
            "  (conv1): Conv1d(8, 64, kernel_size=(5,), stride=(2,))\n",
            "  (conv2): Conv1d(8, 64, kernel_size=(5,), stride=(2,))\n",
            "  (gating): Sigmoid()\n",
            "  (global_max_pool): AdaptiveMaxPool1d(output_size=1)\n",
            "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MalConv(nn.Module):\n",
        "    def __init__(self, input_length=3000000, embedding_dim=8, window_size=5, output_dim=1):\n",
        "        super(MalConv, self).__init__()\n",
        "        self.embed = nn.Embedding(input_length, embedding_dim)\n",
        "        self.conv1 = nn.Conv1d(embedding_dim, 64, kernel_size=window_size, stride=window_size - 3, padding=0)\n",
        "        self.conv2 = nn.Conv1d(embedding_dim, 64, kernel_size=window_size, stride=window_size - 3, padding=0)\n",
        "        self.gating = nn.Sigmoid()\n",
        "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
        "        self.fc1 = nn.Linear(64, 64)\n",
        "        self.fc2 = nn.Linear(64, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        # Convert to (batch_size, channels, length)\n",
        "        x = x.transpose(1, 2)\n",
        "        conv1 = self.conv1(x)\n",
        "        conv2 = self.conv2(x)\n",
        "        # Element-wise multiplication\n",
        "        gated = conv1 * self.gating(conv2)\n",
        "         # Remove the last dimension\n",
        "        global_max_pool = self.global_max_pool(gated).squeeze(2)\n",
        "        fc1 = F.relu(self.fc1(global_max_pool))\n",
        "        fc2 = self.fc2(fc1)\n",
        "        output = self.sigmoid(fc2)\n",
        "        return output\n",
        "\n",
        "\n",
        "model = MalConv()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pihnLcFmbaet"
      },
      "source": [
        "**Step 8:** Partial fit the standardScaler to avoid overloading the memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "H4q5OfK9v9iN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "cbb685c9-ac83-4503-c47c-93266cba37c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MinMaxScaler(feature_range=(0, 1000))"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler(feature_range=(0, 1000))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler(feature_range=(0, 1000))</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "mm = MinMaxScaler(feature_range=(0, 1000))\n",
        "mm.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "B33Oa1sTxdB0"
      },
      "outputs": [],
      "source": [
        "X_train = mm.transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "V_vl5yrex0yY"
      },
      "outputs": [],
      "source": [
        "## Reshape to create 3 channels ##\n",
        "import numpy as np\n",
        "X_train = np.reshape(X_train,(-1,1,2381))\n",
        "y_train = np.reshape(y_train,(-1,1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1iRXFtuvCps"
      },
      "source": [
        "**Load, Tensorize, and Split** The following code takes care of converting the training data into Torch Tensors, and then splits it into 80% training and 20% validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ja3fhJI6qJKN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming MalConv class definition is already provided as above\n",
        "\n",
        "# Convert your numpy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "# Split the data into training and validation sets (80% training, 20% validation)\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train_tensor, y_train_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create TensorDatasets and DataLoaders for training and validation sets\n",
        "train_dataset = TensorDataset(X_train_split, y_train_split)\n",
        "val_dataset = TensorDataset(X_val_split, y_val_split)\n",
        "\n",
        "batch_size = 128  # Adjust based on your GPU memory\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zMgth6McCqV"
      },
      "source": [
        "> **Task 2:** Complete the following code to train the model on the GPU for 15 epochs, with a batch size of 64. If you are on Google Colab, don't forget to change the kernel in Runtime -> Change runtime type -> T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv7piF7dp0lm",
        "outputId": "284aff83-a4aa-47a0-ae4e-0911b6cadba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.2515886168877284\n",
            "Validation Loss: 0.17497112855632255\n",
            "Model checkpoint saved to drive/MyDrive/vMalConv/model_epoch_1.pt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Initialize the MalConv model\n",
        "model = MalConv()\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
        "\n",
        "# Directory to save model checkpoints\n",
        "save_dir = \"drive/MyDrive/vMalConv/\"\n",
        "\n",
        "# Training Loop with Validation\n",
        "num_epochs = 1  # Adjust the number of epochs as needed\n",
        "train_loss_ = []\n",
        "val_loss_ = []\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "        inputs = inputs.view(inputs.size(0), -1)\n",
        "        #print(inputs.shape)\n",
        "        outputs = model(inputs)\n",
        "        #print(outputs)\n",
        "        labels = labels.squeeze(2)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader)}')\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    train_loss_.append(epoch_loss)\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            inputs = inputs.view(inputs.size(0), -1)\n",
        "            outputs = model(inputs)\n",
        "            #print(outputs)\n",
        "            labels = labels.squeeze(2)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "    print(f'Validation Loss: {val_loss/len(val_loader)}')\n",
        "    epoch_val_loss = val_loss/len(val_loader)\n",
        "    val_loss_.append(epoch_val_loss)\n",
        "\n",
        "    # Save checkpoint every 5 epochs\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        checkpoint_path = os.path.join(save_dir, f'model_epoch_{epoch+1}.pt')\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f'Model checkpoint saved to {checkpoint_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2VNvPecJFwwE"
      },
      "outputs": [],
      "source": [
        "#print(inputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obToo1WZtD4m"
      },
      "source": [
        "**Task 3:** Complete the following code to evaluate your trained model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump\n",
        "dump(mm, 'minmax_scaler_model.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGdivUQjEvc3",
        "outputId": "51ca81e1-e51f-4437-db07-b6330c4ecfc2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['minmax_scaler_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bZ0Iif_m8UiL"
      },
      "outputs": [],
      "source": [
        "labelrows = (y_test != -1)\n",
        "X_test = X_test[labelrows]\n",
        "y_test = y_test[labelrows]\n",
        "X_test = X_test[:1000]\n",
        "y_test = y_test[:1000]\n",
        "\n",
        "# Scalar\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "mm = MinMaxScaler(feature_range=(0, 1000))\n",
        "mm.fit(X_test)\n",
        "X_test = mm.transform(X_test)\n",
        "#X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cx33gO0_8YQN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "X_test = np.reshape(X_test,(-1,1,2381))\n",
        "y_test = np.reshape(y_test,(-1,1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "83wqvS9jqppe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a730b4ea-b4a9-4ec6-e8d5-484d2740b73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4880\n",
            "Precision: 0.4869\n",
            "Recall:0.9938\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Convert test data to PyTorch tensors\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create a TensorDataset and DataLoader for test data\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#device = torch.device('cpu')\n",
        "#model = MalConv()\n",
        "#model.load_state_dict(torch.load(\"/content/drive/MyDrive/vMalConv/model_epoch_5.pt\", map_location=device))\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "#print(model)\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      inputs = inputs.view(inputs.size(0), -1)\n",
        "      #print(inputs)\n",
        "      outputs = model(inputs)\n",
        "      #print(outputs)\n",
        "      #print(outputs.shape)\n",
        "      # Fix the issue by ensuring that the threshold value is a float\n",
        "      outputs = torch.where(outputs > 0.02, torch.tensor([1.0], dtype=torch.float32), torch.tensor([0.0], dtype=torch.float32))\n",
        "      labels = labels.squeeze(2)\n",
        "      y_pred.extend(outputs.cpu().numpy())\n",
        "      y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy, precision, and recall\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall:{recall:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_pred"
      ],
      "metadata": {
        "id": "u6nk8QT34WWt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_true"
      ],
      "metadata": {
        "id": "wLAr4g6c4nGR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "\n",
        "# Convert predictions to binary values (0 or 1)\n",
        "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_binary)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_true, y_pred_binary)\n",
        "\n",
        "# Print F1 score\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "LKPVxedHbJkL",
        "outputId": "10261964-4ca9-4cc0-f853-56cac2f0417b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.6535859269282814\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from colorama import Back, Fore, Style\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_true, y_pred_binary)\n",
        "\n",
        "# Define color and background color\n",
        "color = Fore.WHITE  # Text color\n",
        "bg_color = Back.GREEN  # Background color\n",
        "\n",
        "# Print F1 score in a colorful box\n",
        "print(f\"{bg_color}{color}F1 Score: {f1:.4f}{Style.RESET_ALL}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PrHhzp4dpxZ",
        "outputId": "d981c5e6-4473-4f71-a13c-d113769acd29"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[42m\u001b[37mF1 Score: 0.6536\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6fLYYxps91N"
      },
      "source": [
        "**Task 4:** Comment on the results in this text box. Based on the results:\n",
        "\n",
        "- The model has a high recall but relatively low precision, suggesting it's good at identifying positive instances but also produces a significant number of false positives.\n",
        "- The overall accuracy is low, likely due to the imbalance between the classes or the high number of false positives.\n",
        "- The F1 score indicates a reasonable balance between precision and recall, but there is still room for improvement.\n",
        "\n",
        "In conclusion, while the model shows promise in identifying positive instances, there's a need for further optimization to reduce false positives and improve overall accuracy. Additional analysis and adjustments to the model may be necessary to achieve better performance."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}