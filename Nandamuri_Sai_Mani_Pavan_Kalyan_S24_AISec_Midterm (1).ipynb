{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esNWMzKrVuWc"
      },
      "source": [
        "**Revised on 3/5/2024: Changed source files**\n",
        "\n",
        "This is the skeleton code for Task 1 of the midterm project. The files that are downloaded in step 4 are based on the [Ember 2018 dataset](https://arxiv.org/abs/1804.04637), and contain the features (and corresponding labels) extracted from 1 million PE files, split into 80\\% training and 20\\% test datasets. The code used for for feature extraction is available [here](https://colab.research.google.com/drive/16q9bOlCmnTquPtVXVzxUj4ZY1ORp10R2?usp=sharing). However, the preprocessing and featurization process may take up to 3 hours on Google Colab. Hence, I recommend using the processed datasets (Step 4) to speed up your development.\n",
        "\n",
        "Also, note that there is a new optional step 8.5 - To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets.\n",
        "\n",
        "**Step 1:** Mount your Google Drive by clicking on \"Mount Drive\" in the Files section (panel to the left of this text.)\n",
        "\n",
        "**Step 2:** Go to Runtime -> Change runtime type and select T4 GPU.\n",
        "\n",
        "**Step 3:** Create a folder in your Google Drive, and rename it to \"vMalConv\"\n",
        "\n",
        "**Step 4:** Download the pre-processed training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUq_FZwmZegw",
        "outputId": "af3c2505-840e-48dc-bab4-4dfe4a16a662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-14 01:46:35--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 52.217.139.233, 52.217.49.236, 52.217.72.236, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|52.217.139.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7619200000 (7.1G) [binary/octet-stream]\n",
            "Saving to: ‘X_train.dat’\n",
            "\n",
            "X_train.dat          51%[=========>          ]   3.63G  49.1MB/s    eta 80s    ^C\n",
            "--2024-03-14 01:47:59--  https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\n",
            "Resolving dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)... 52.217.129.41, 54.231.194.217, 52.217.91.172, ...\n",
            "Connecting to dsci6015s24-midterm.s3.amazonaws.com (dsci6015s24-midterm.s3.amazonaws.com)|52.217.129.41|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1904800000 (1.8G) [binary/octet-stream]\n",
            "Saving to: ‘X_test.dat’\n",
            "\n",
            "X_test.dat            4%[                    ]  76.34M  51.6MB/s               "
          ]
        }
      ],
      "source": [
        "## ~8GB\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_train.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/X_test.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_train.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/y_test.dat\n",
        "!wget https://dsci6015s24-midterm.s3.amazonaws.com/v2/metadata.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AQ_JdZKfG7Q-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec5f5fd-1976-4bf1-b878-a8b66971d630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V958PbDW3H0"
      },
      "source": [
        "**Step 5:** Copy the downloaded files to vMalConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llip77F3amma"
      },
      "outputs": [],
      "source": [
        "!cp /content/X_train.dat /content/drive/MyDrive/vMalConv/X_train.dat\n",
        "!cp /content/X_test.dat /content/drive/MyDrive/vMalConv/X_test.dat\n",
        "!cp /content/y_train.dat /content/drive/MyDrive/vMalConv/y_train.dat\n",
        "!cp /content/y_test.dat /content/drive/MyDrive/vMalConv/y_test.dat\n",
        "!cp /content/metadata.csv /content/drive/MyDrive/vMalConv/metadata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbRilyqTXnrE"
      },
      "source": [
        "**Step 6:** Download and install Ember:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "76bc7PEmlwKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d79ed7-a82c-4c5c-e95e-2a93dfd59149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PFGimenez/ember.git\n",
            "  Cloning https://github.com/PFGimenez/ember.git to /tmp/pip-req-build-p0c2950d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/PFGimenez/ember.git /tmp/pip-req-build-p0c2950d\n",
            "  Resolved https://github.com/PFGimenez/ember.git to commit 3b82fe63069884882e743af725d29cc2a67859f1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ember\n",
            "  Building wheel for ember (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ember: filename=ember-0.1.0-py3-none-any.whl size=13050 sha256=3ba1b5e31d746ec019cb6e8a4daac386663347deea079f6552cf126f9ce7315f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-60t9cxwl/wheels/8f/69/f9/1917c8df03b25fe53e8e2f6cb2c9f61a43dec179b19b10ab9f\n",
            "Successfully built ember\n",
            "Installing collected packages: ember\n",
            "Successfully installed ember-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/PFGimenez/ember.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8aRVMSwCQT7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbdae4c1-8f1f-49ee-e889-bf366d258289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lief\n",
            "  Downloading lief-0.14.1-cp310-cp310-manylinux_2_28_x86_64.manylinux_2_27_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lief\n",
            "Successfully installed lief-0.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install lief"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ember"
      ],
      "metadata": {
        "id": "kvzi-Xi0-Qt7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXym5qd8Yv8f"
      },
      "source": [
        "**Step 7:** Read vectorized features from the data files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GfcHyoTsmCFH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "b0abd37e-7648-4eaf-99ef-52fe58748f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
            "WARNING:   lief version 0.14.1-bae887e0 found instead. There may be slight inconsistencies\n",
            "WARNING:   in the feature calculations.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c2f1e7c04487>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0member\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0member\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_vectorized_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/MyDrive/vMalConv/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmetadata_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0member\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/MyDrive/vMalConv/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ember/__init__.py\u001b[0m in \u001b[0;36mread_metadata\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mRead\u001b[0m \u001b[0man\u001b[0m \u001b[0malready\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mits\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \"\"\"\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"metadata.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
          ]
        }
      ],
      "source": [
        "import ember\n",
        "X_train, y_train, X_test, y_test = ember.read_vectorized_features(\"drive/MyDrive/vMalConv/\")\n",
        "metadata_dataframe = ember.read_metadata(\"drive/MyDrive/vMalConv/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTRCz7m7Z7EH"
      },
      "source": [
        "**Step 8:** Get rid of rows with no labels."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "5DEQWb07NGgA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj63lcvin44q"
      },
      "outputs": [],
      "source": [
        "labelrows = (y_train != -1)\n",
        "X_train = X_train[labelrows]\n",
        "\n",
        "y_train = y_train[labelrows]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLZmJ9hAkzQP"
      },
      "outputs": [],
      "source": [
        "print(\"X_train.shape:\",X_train.shape)\n",
        "print(\"y_train.shape:\",y_train.shape)\n",
        "print(\"X_test.shape:\",X_test.shape)\n",
        "print(\"y_test.shape:\",y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train[:60000]\n",
        "\n",
        "y_train = y_train[:60000]"
      ],
      "metadata": {
        "id": "r0dKNMFkc257"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train.shape:\",X_train.shape)\n",
        "print(\"y_train.shape:\",y_train.shape)"
      ],
      "metadata": {
        "id": "VZ7jUWFadBd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save X_train and y_train as Pickle files\n",
        "with open('X_train.pkl', 'wb') as file:\n",
        "    pickle.dump(X_train, file)\n",
        "\n"
      ],
      "metadata": {
        "id": "h-WAOCvzM6jI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/X_train.pkl /content/drive/MyDrive/vMalConv/X_train.pkl"
      ],
      "metadata": {
        "id": "3t6es4K8YgpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train= None\n",
        "# # # Load X_train and y_train from Pickle files\n",
        "with open('/content/drive/MyDrive/vMalConv/X_train.pkl', 'rb') as file:\n",
        "    X_train = pickle.load(file)"
      ],
      "metadata": {
        "id": "Yv6kiT1oYmx6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('y_train.pkl', 'wb') as file:\n",
        "    pickle.dump(y_train, file)"
      ],
      "metadata": {
        "id": "oigWqmxyYarM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!cp /content/y_train.pkl /content/drive/MyDrive/vMalConv/y_train.pkl"
      ],
      "metadata": {
        "id": "JFFkwIS_NUc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mVG59AGooyC5"
      },
      "outputs": [],
      "source": [
        "y_train=None\n",
        "\n",
        "with open('/content/drive/MyDrive/vMalConv/y_train.pkl', 'rb') as file:\n",
        "    y_train = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKoXSzp59RN-"
      },
      "source": [
        "**Optional Step 8.5:** To speed up your experiments, you may want to sample the original dataset of 800k training samples and 200k test samples to smaller datasets. You can use the [Pandas Dataframe sample() method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html), or come up with your own sampling methodology. Be mindful of the fact that the database is heavily imbalanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-X9wwv_n9QkY"
      },
      "outputs": [],
      "source": [
        "# Number of samples to keep for each class\n",
        "samples_per_class = 60000\n",
        "\n",
        "# Unique classes in y_train and their counts\n",
        "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Empty arrays to store sampled data\n",
        "sampled_X_train = []\n",
        "sampled_y_train = []\n",
        "\n",
        "# Iterate through each class\n",
        "for class_label, count in zip(unique_classes, class_counts):\n",
        "    # Get indices of samples belonging to the current class\n",
        "    class_indices = np.where(y_train == class_label)[0]\n",
        "\n",
        "    # Randomly sample\n",
        "    sampled_indices = np.random.choice(class_indices, size=min(samples_per_class, count), replace=False)\n",
        "\n",
        "    # Append sampled data to the result arrays\n",
        "    sampled_X_train.append(X_train[sampled_indices])\n",
        "    sampled_y_train.append(y_train[sampled_indices])\n",
        "\n",
        "# Concatenate the sampled data\n",
        "sampled_X_train = np.concatenate(sampled_X_train, axis=0)\n",
        "sampled_y_train = np.concatenate(sampled_y_train, axis=0)\n",
        "\n",
        "# Shuffle the sampled data (optional)\n",
        "shuffle_indices = np.random.permutation(len(sampled_X_train))\n",
        "sampled_X_train = sampled_X_train[shuffle_indices]\n",
        "sampled_y_train = sampled_y_train[shuffle_indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sampled_X_train_100k.pkl', 'wb') as file:\n",
        "    pickle.dump(sampled_X_train, file)\n",
        "\n",
        "with open('sampled_y_train_100k.pkl', 'wb') as file:\n",
        "    pickle.dump(sampled_y_train, file)"
      ],
      "metadata": {
        "id": "vIuN0i55NqRp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/sampled_X_train_100k.pkl /content/drive/MyDrive/vMalConv/sampled_X_train_60k.pkl\n",
        "!cp /content/sampled_y_train_100k.pkl /content/drive/MyDrive/vMalConv/sampled_y_train_60k.pkl"
      ],
      "metadata": {
        "id": "Wg4Nko9DNuQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train  = None, None\n",
        "\n",
        "# # Load X_train and y_train from Pickle files\n",
        "with open('/content/drive/MyDrive/vMalConv/sampled_X_train_60k.pkl', 'rb') as file:\n",
        "    X_train = pickle.load(file)\n",
        "\n",
        "with open('/content/drive/MyDrive/vMalConv/sampled_y_train_60k.pkl', 'rb') as file:\n",
        "    y_train = pickle.load(file)"
      ],
      "metadata": {
        "id": "s01N9WLZNh5i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1bRlBWlaQdd"
      },
      "source": [
        "> **Task 1:** Complete the following code to build the architecture of MalConv in PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MalConv(nn.Module):\n",
        "    def __init__(self, input_length=3000000, embedding_dim=8, window_size=5, output_dim=1):\n",
        "        super(MalConv, self).__init__()\n",
        "        self.embed = nn.Embedding(input_length, embedding_dim)\n",
        "        self.conv1 = nn.Conv1d(embedding_dim, 64, kernel_size=window_size, stride=window_size - 3, padding=0)\n",
        "        self.conv2 = nn.Conv1d(embedding_dim, 64, kernel_size=window_size, stride=window_size - 3, padding=0)\n",
        "        self.gating = nn.Sigmoid()\n",
        "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
        "        self.fc1 = nn.Linear(64, 64)\n",
        "        self.fc2 = nn.Linear(64, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        # Convert to (batch_size, channels, length)\n",
        "        x = x.transpose(1, 2)\n",
        "        conv1 = self.conv1(x)\n",
        "        conv2 = self.conv2(x)\n",
        "        # Element-wise multiplication\n",
        "        gated = conv1 * self.gating(conv2)\n",
        "         # Remove the last dimension\n",
        "        global_max_pool = self.global_max_pool(gated).squeeze(2)\n",
        "        fc1 = F.relu(self.fc1(global_max_pool))\n",
        "        fc2 = self.fc2(fc1)\n",
        "        output = self.sigmoid(fc2)\n",
        "        return output\n",
        "\n",
        "\n",
        "model = MalConv()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoD0gkUxoux5",
        "outputId": "c51572b0-4f35-4ee7-c138-c28c3460c67e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MalConv(\n",
            "  (embed): Embedding(3000000, 8)\n",
            "  (conv1): Conv1d(8, 64, kernel_size=(5,), stride=(2,))\n",
            "  (conv2): Conv1d(8, 64, kernel_size=(5,), stride=(2,))\n",
            "  (gating): Sigmoid()\n",
            "  (global_max_pool): AdaptiveMaxPool1d(output_size=1)\n",
            "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pihnLcFmbaet"
      },
      "source": [
        "**Step 8:** Partial fit the standardScaler to avoid overloading the memory:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "#we want to standardize the feature, we can use MinMaxScaler like we have used in previous calsses\n",
        "MM= MinMaxScaler(feature_range=(0, 1000))\n",
        "MM.fit(X_train)\n",
        "X_train = MM.transform(X_train)\n",
        "# X_test=ss.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "f4v_Ne9KqolJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "8GfSZrmcrRRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rehutW_q3r-p"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w95a1XO34TXi"
      },
      "source": [
        "No Need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_vl5yrex0yY"
      },
      "outputs": [],
      "source": [
        "## Reshape to create 3 channels ##\n",
        "#import numpy as np\n",
        "#X_train = np.reshape(X_train,(-1,1,2381))\n",
        "#y_train = np.reshape(y_train,(-1,1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1iRXFtuvCps"
      },
      "source": [
        "**Load, Tensorize, and Split** The following code takes care of converting the training data into Torch Tensors, and then splits it into 80% training and 20% validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ja3fhJI6qJKN"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert your numpy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.int)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "# Split the data into training and validation sets (80% training, 20% validation)\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train_tensor, y_train_tensor, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Create TensorDatasets and DataLoaders for training and validation sets\n",
        "train_dataset = TensorDataset(X_train_split, y_train_split)\n",
        "val_dataset = TensorDataset(X_val_split, y_val_split)\n",
        "\n",
        "batch_size = 4000  # Adjust based on GPU memory\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zMgth6McCqV"
      },
      "source": [
        "> **Task 2:** Complete the following code to train the model on the GPU for 15 epochs, with a batch size of 64. If you are on Google Colab, don't forget to change the kernel in Runtime -> Change runtime type -> T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "del X_train\n",
        "del X_train_split\n",
        "del X_val_split\n",
        "del y_train\n",
        "del y_train_split\n",
        "del y_val_split\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4q4r6jKXYxp",
        "outputId": "bb6d137e-6d06-411a-e47b-4d99782c3abb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = MalConv()\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Directory to save model checkpoints\n",
        "save_dir = \"./model_checkpoints\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Training Loop with Validation\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Function to check early stopping\n",
        "def early_stopping(val_losses, patience=5):\n",
        "    if len(val_losses) < patience + 1:\n",
        "        return False\n",
        "    return all(val_losses[-1] >= val_losses[-i - 1] for i in range(1, patience + 1))\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "early_stopping_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(epoch_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    epoch_val_loss = val_loss / len(val_loader)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Training Loss: {epoch_loss}, Validation Loss: {epoch_val_loss}')\n",
        "\n",
        "    # Check for early stopping\n",
        "    if early_stopping(val_losses, patience):\n",
        "        print(f'Early stopping after {epoch+1} epochs without improvement.')\n",
        "        break\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        checkpoint_path = os.path.join(save_dir, f'model_epoch_{epoch+1}.pt')\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f'Model checkpoint saved to {checkpoint_path}')\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXXW3PbLtpeC",
        "outputId": "d215c3ff-8235-44e1-96ab-d59098c3f1bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.40727420647939044, Validation Loss: 0.29306311160326004\n",
            "Epoch 2, Training Loss: 0.3057103256384532, Validation Loss: 0.29852309823036194\n",
            "Epoch 3, Training Loss: 0.2938297837972641, Validation Loss: 0.28966740518808365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test  = None, None\n",
        "\n",
        "# # Load X_train and y_train from Pickle files\n",
        "with open('/content/drive/MyDrive/vMalConv/X_test.pkl', 'rb') as file:\n",
        "    X_test = pickle.load(file)\n",
        "\n",
        "with open('/content/drive/MyDrive/vMalConv/y_test.pkl', 'rb') as file:\n",
        "    y_test = pickle.load(file)"
      ],
      "metadata": {
        "id": "pB-Qrjh8ADw1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_test[:1000]\n",
        "y_test = y_test[:1000]"
      ],
      "metadata": {
        "id": "qRU7xZ23IFt1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CbEy6GnHfRBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('X_test.pkl', 'wb') as file:\n",
        "    pickle.dump(X_test, file)"
      ],
      "metadata": {
        "id": "5FNr1AlBe7-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('y_test.pkl', 'wb') as file:\n",
        "    pickle.dump(y_test, file)"
      ],
      "metadata": {
        "id": "UFhr1yfQfv4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/X_test.pkl /content/drive/MyDrive/vMalConv/X_test.pkl"
      ],
      "metadata": {
        "id": "kHjqYjB-fSQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/y_test.pkl /content/drive/MyDrive/vMalConv/y_test.pkl"
      ],
      "metadata": {
        "id": "LjsrMWn3fzVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test  = None, None\n",
        "\n",
        "# # Load X_test and y_test from Pickle files\n",
        "with open('/content/drive/MyDrive/vMalConv/X_test.pkl', 'rb') as file:\n",
        "    X_test = pickle.load(file)\n",
        "\n",
        "with open('/content/drive/MyDrive/vMalConv/y_test.pkl', 'rb') as file:\n",
        "    y_test = pickle.load(file)"
      ],
      "metadata": {
        "id": "usZUN9kmfnTl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = MM.transform(X_test)"
      ],
      "metadata": {
        "id": "LY6Ktd0GAJLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs.shape)"
      ],
      "metadata": {
        "id": "iBJgokOwgbE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Convert test data to PyTorch tensors\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.int)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create a TensorDataset and DataLoader for test data\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Lists to store model predictions and actual labels\n",
        "predictions = []\n",
        "labels_list = []  # Renamed to avoid confusion with the loop variable\n",
        "\n"
      ],
      "metadata": {
        "id": "v37l4CKY2qMZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for inputs, labels_batch in test_loader:\n",
        "        #inputs = inputs.to(device)  # Move inputs to the same device as model\n",
        "        outputs = model(inputs)\n",
        "        predicted = torch.round(outputs,dim=1)  # Convert probabilities to binary predictions (0 or 1)\n",
        "\n",
        "        # Store predictions and labels\n",
        "        predictions.extend(predicted.numpy())\n",
        "        labels_list.extend(labels_batch.numpy())\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(labels_list, predictions)\n",
        "precision = precision_score(labels_list, predictions)\n",
        "recall = recall_score(labels_list, predictions)\n",
        "\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "9Ju9wPq5lG1R",
        "outputId": "0747acbf-d453-4eb0-de70-fe109f2c62da"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-f18d0764aa3b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m#inputs = inputs.to(device)  # Move inputs to the same device as model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert probabilities to binary predictions (0 or 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-2f090f84fed6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Convert to (batch_size, channels, length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2235\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "accuracy = accuracy_score(labels_list, predictions)\n",
        "precision = precision_score(labels_list, predictions)\n",
        "recall = recall_score(labels_list, predictions)\n",
        "cm = confusion_matrix(labels_list, predictions)\n",
        "\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.matshow(cm, cmap=plt.cm.Blues)\n",
        "plt.colorbar()\n",
        "class_names = ['Benign', 'Malware']\n",
        "plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
        "plt.yticks(range(len(class_names)), class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "for i in range(len(cm)):\n",
        "  for j in range(len(cm)):\n",
        "    plt.text(j, i, cm[i, j], ha='center', va='center', fontsize=8)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rTnHGA9EkWk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model parameters\n",
        "model_weights_path = \"malConv_model_weights.pt\"\n",
        "torch.save(model.state_dict(), model_weights_path)\n",
        "print(f\"Model weights saved to {model_weights_path}\")\n",
        "# Save a checkpoint including model state, optimizer state, and other info\n",
        "checkpoint_path = os.path.join(save_dir, \"model_final_checkpoint.pt\")\n",
        "torch.save({\n",
        "    'epoch': num_epochs,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'last_loss': loss.item(),\n",
        "    # Include any other information you need\n",
        "}, checkpoint_path)\n",
        "print(f\"Full model checkpoint saved to {checkpoint_path}\")"
      ],
      "metadata": {
        "id": "j3_79MkhyTGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lah"
      ],
      "metadata": {
        "id": "pGrJYSEAv4be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reload model\n",
        "\n",
        "# Create a new model instance\n",
        "model = MalConv()\n",
        "\n",
        "# Load the weights\n",
        "model.load_state_dict(torch.load(\"malConv_model_weights.pt\"))\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "# #  load the full checkpoint\n",
        "# checkpoint = torch.load(\"/content/model_checkpoints/model_final_checkpoint.pt\")\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust hyperparameters as needed\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "metadata": {
        "id": "ta3smb-YzS91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6fLYYxps91N"
      },
      "source": [
        "**Task 4:** Comment on the results in this text box."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}